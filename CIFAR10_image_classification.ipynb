{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import time\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test)=tf.keras.datasets.cifar10.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_names=['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train=x_train.transpose(0, 2, 3, 1)\n",
    "x_test=x_test.transpose(0, 2, 3, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(x):\n",
    "    \"\"\"\n",
    "        argument\n",
    "            - x: input image data in numpy array [32, 32, 3]\n",
    "        return\n",
    "            - normalized x \n",
    "    \"\"\"\n",
    "    min_val = np.min(x)\n",
    "    max_val = np.max(x)\n",
    "    x = (x-min_val) / (max_val-min_val)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encode(x):\n",
    "    \"\"\"\n",
    "        argument\n",
    "            - x: a list of labels\n",
    "        return\n",
    "            - one hot encoding matrix (number of labels, number of class)\n",
    "    \"\"\"\n",
    "    encoded = np.zeros((len(x), 10))\n",
    "    \n",
    "    for idx, val in enumerate(x):\n",
    "        encoded[idx][val] = 1\n",
    "    \n",
    "    return encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize\n",
    "x_train = normalize(x_train)\n",
    "x_test = normalize(x_test)\n",
    "\n",
    "\n",
    "y_train=one_hot_encode(y_train)\n",
    "y_test=one_hot_encode(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_epochs = 150\n",
    "batch_size = 128\n",
    "learning_rate = 0.001\n",
    "display_step=10\n",
    "n_hidden=200\n",
    "\n",
    "input_size=3072\n",
    "output_size=10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "def layer(x, weight_shape, bias_shape, activation):\n",
    "    \n",
    "    \"\"\"\n",
    "    Defines the network layers\n",
    "    input:\n",
    "        - x: input vector of the layer\n",
    "        - weight_shape: shape the the weight maxtrix\n",
    "        - bias_shape: shape of the bias vector\n",
    "    output:\n",
    "        - output vector of the layer after the matrix multiplication and non linear transformation\n",
    "    \"\"\"\n",
    "    \n",
    "    # comes from the study by He et al. for ReLU layers\n",
    "    w_std = (2.0/weight_shape[0])**0.5\n",
    "    #print(weight_shape[0])\n",
    "    #w_std = 0.5;\n",
    "\n",
    "    #initialization of the weights\n",
    "    #you can try either\n",
    "    w_0 = tf.random_normal_initializer(stddev=w_std)\n",
    "    #w_0 = tf.random_uniform_initializer(minval=-1,maxval=1)\n",
    "\n",
    "    b_0 = tf.constant_initializer(value=0)\n",
    "    \n",
    "    W = tf.get_variable(\"W\", weight_shape, initializer=w_0)\n",
    "    b = tf.get_variable(\"b\", bias_shape,   initializer=b_0)\n",
    "    \n",
    "    print('Weight Matrix:', W)\n",
    "    print('Bias Vector:', b)\n",
    "    \n",
    "    \n",
    "    if activation=='relu':\n",
    "        return tf.nn.relu(tf.matmul(x, W) + b)\n",
    "    elif activation== 'linear':\n",
    "        return tf.matmul(x, W) + b\n",
    "    elif activation=='tanh':\n",
    "        return tf.nn.tanh(tf.matmul(x, W) + b)\n",
    "    elif activation=='sigmoid':\n",
    "        return tf.nn.sigmoid(tf.matmul(x, W) + b)\n",
    "    elif activation=='leakyrelu':\n",
    "        return tf.nn.leaky_relu(tf.matmul(x, W) + b)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(x, desired_layer=4):\n",
    "    \"\"\"\n",
    "    define the whole network (2 hidden layers + output layers)\n",
    "    input:\n",
    "        - a batch of pictures \n",
    "        (input shape = (batch_size*image_size))\n",
    "    output:\n",
    "        - a batch vector corresponding to the logits predicted by the network\n",
    "        (output shape = (batch_size*output_size)) \n",
    "    \"\"\"\n",
    "    \n",
    "    hidden={}\n",
    "    #activation=['sigmoid', 'sigmoid', 'reLU', 'tanh', 'leakyrelu']\n",
    "    \n",
    "    with tf.variable_scope('hidden_layer_1'):\n",
    "            hidden[1] = layer(x, [input_size, n_hidden], [n_hidden], 'leakyrelu')\n",
    "\n",
    "    range_=np.arange(2, desired_layer+1, 1)\n",
    "    \n",
    "    for i in range_:\n",
    "        name_='hidden_layer_'+str(i)\n",
    "        \n",
    "        with tf.variable_scope(name_):\n",
    "           \n",
    "            hidden[i] = layer(hidden[i-1], [n_hidden, n_hidden], [n_hidden], 'leakyrelu')\n",
    "            #print([input_size, n_hidden_1])\n",
    "     \n",
    "    \n",
    "    with tf.variable_scope(\"output\"):\n",
    "        output = layer(hidden[desired_layer], [n_hidden, output_size], [output_size], 'leakyrelu')\n",
    "        #print([n_hidden_2, output_size])\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_2(output, y):\n",
    "    \"\"\"\n",
    "    Computes softmax cross entropy between logits and labels and then the loss \n",
    "    \n",
    "    intput:\n",
    "        - output: the output of the inference function \n",
    "        - y: true value of the sample batch\n",
    "         \n",
    "        the two have the same shape (batch_size * num_of_classes)\n",
    "    output:\n",
    "        - loss: loss of the corresponding batch (scalar tensor)\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    #mean square error\n",
    "    #loss = tf.reduce_mean(tf.reduce_sum(tf.square(y-output)))\n",
    "    \n",
    "    #Computes softmax cross entropy between logits and labels.\n",
    "    xentropy = tf.nn.softmax_cross_entropy_with_logits(logits=output, labels=y)\n",
    "    loss = tf.reduce_mean(xentropy)\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training(cost, global_step):\n",
    "    \"\"\"\n",
    "    defines the necessary elements to train the network\n",
    "    \n",
    "    intput:\n",
    "        - cost: the cost is the loss of the corresponding batch\n",
    "        - global_step: number of batch seen so far, it is incremented by one \n",
    "        each time the .minimize() function is called\n",
    "    \"\"\"\n",
    "    \n",
    "    tf.summary.scalar(\"cost\", cost)\n",
    "    \n",
    "    # tf.train.RMSPropOptimizer\n",
    "    # will use different optimization routines \n",
    "    optimizer = tf.train.RMSPropOptimizer(learning_rate)\n",
    "    train_op = optimizer.minimize(cost, global_step=global_step)\n",
    "    \n",
    "    return train_op"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(output, y):\n",
    "    \"\"\"\n",
    "    evaluates the accuracy on the validation set \n",
    "    input:\n",
    "        -output: prediction vector of the network for the validation set\n",
    "        -y: true value for the validation set\n",
    "    output:\n",
    "        - accuracy: accuracy on the validation set (scalar between 0 and 1)\n",
    "    \"\"\"\n",
    "    #correct prediction is a binary vector which equals one when the output and y match\n",
    "    #otherwise the vector equals 0\n",
    "    #tf.cast: change the type of a tensor into another one\n",
    "    #then, by taking the mean of the tensor, we directly have the average score, so the accuracy\n",
    "    \n",
    "    correct_prediction = tf.equal(tf.argmax(output, 1), tf.argmax(y, 1))\n",
    "    \n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "    tf.summary.scalar(\"validation_error\", (1.0 - accuracy))\n",
    "\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train2=x_train.reshape(50000, 32*3*32)\n",
    "x_test2=x_test.reshape(10000, 32*3*32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_validation, y_train, y_validation=train_test_split(x_train2, y_train, test_size=0.12, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((50000, 3072), (10000, 3072))"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train2.shape, x_test2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "def next_batch(num, data, labels):\n",
    "    '''\n",
    "    Return a total of `num` random samples and labels. \n",
    "    '''\n",
    "    idx = np.arange(0 , len(data))\n",
    "    np.random.shuffle(idx)\n",
    "    idx = idx[:num]\n",
    "    data_shuffle = [data[i] for i in idx]\n",
    "    labels_shuffle = [labels[i] for i in idx]\n",
    "\n",
    "    return np.asarray(data_shuffle), np.asarray(labels_shuffle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weight Matrix: <tf.Variable 'multi_layer/hidden_layer_1/W:0' shape=(3072, 200) dtype=float32_ref>\n",
      "Bias Vector: <tf.Variable 'multi_layer/hidden_layer_1/b:0' shape=(200,) dtype=float32_ref>\n",
      "Weight Matrix: <tf.Variable 'multi_layer/hidden_layer_2/W:0' shape=(200, 200) dtype=float32_ref>\n",
      "Bias Vector: <tf.Variable 'multi_layer/hidden_layer_2/b:0' shape=(200,) dtype=float32_ref>\n",
      "Weight Matrix: <tf.Variable 'multi_layer/hidden_layer_3/W:0' shape=(200, 200) dtype=float32_ref>\n",
      "Bias Vector: <tf.Variable 'multi_layer/hidden_layer_3/b:0' shape=(200,) dtype=float32_ref>\n",
      "Weight Matrix: <tf.Variable 'multi_layer/hidden_layer_4/W:0' shape=(200, 200) dtype=float32_ref>\n",
      "Bias Vector: <tf.Variable 'multi_layer/hidden_layer_4/b:0' shape=(200,) dtype=float32_ref>\n",
      "Weight Matrix: <tf.Variable 'multi_layer/hidden_layer_5/W:0' shape=(200, 200) dtype=float32_ref>\n",
      "Bias Vector: <tf.Variable 'multi_layer/hidden_layer_5/b:0' shape=(200,) dtype=float32_ref>\n",
      "Weight Matrix: <tf.Variable 'multi_layer/hidden_layer_6/W:0' shape=(200, 200) dtype=float32_ref>\n",
      "Bias Vector: <tf.Variable 'multi_layer/hidden_layer_6/b:0' shape=(200,) dtype=float32_ref>\n",
      "Weight Matrix: <tf.Variable 'multi_layer/output/W:0' shape=(200, 10) dtype=float32_ref>\n",
      "Bias Vector: <tf.Variable 'multi_layer/output/b:0' shape=(10,) dtype=float32_ref>\n",
      "Epoch: 000 cost function= 2.2708627  Validation Error: 0.7246666550636292\n",
      "Epoch: 010 cost function= 1.1547153  Validation Error: 0.551499992609024\n",
      "Epoch: 020 cost function= 0.8340561  Validation Error: 0.5206666588783264\n",
      "Epoch: 030 cost function= 0.5971997  Validation Error: 0.5041666626930237\n",
      "Epoch: 040 cost function= 0.4326150  Validation Error: 0.4868333339691162\n",
      "Epoch: 050 cost function= 0.3288300  Validation Error: 0.4909999966621399\n",
      "Epoch: 060 cost function= 0.2509208  Validation Error: 0.5106666684150696\n",
      "Epoch: 070 cost function= 0.2058517  Validation Error: 0.4866666793823242\n",
      "Epoch: 080 cost function= 0.1735885  Validation Error: 0.49449998140335083\n",
      "Epoch: 090 cost function= 0.1471485  Validation Error: 0.5071666538715363\n",
      "Epoch: 100 cost function= 0.1364199  Validation Error: 0.4951666593551636\n",
      "Epoch: 110 cost function= 0.1266155  Validation Error: 0.5069999992847443\n",
      "Epoch: 120 cost function= 0.1197337  Validation Error: 0.49916666746139526\n",
      "Epoch: 130 cost function= 0.1096082  Validation Error: 0.4978333115577698\n",
      "Epoch: 140 cost function= 0.1096809  Validation Error: 0.5153333246707916\n",
      "Optimization Finished!\n",
      "Test Accuracy: 0.5017\n",
      "Execution time (seconds) was 1055.863\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    \n",
    "    start_time = time.time()\n",
    "    #please, make sure you changed for your own path \n",
    "    log_files_path = '/Users/Renaissance/Desktop/'\n",
    "\n",
    "    desired_layer=6\n",
    "    \n",
    "    with tf.Graph().as_default():\n",
    "        \n",
    "        with tf.variable_scope(\"multi_layer\"):\n",
    "            #neural network definition \n",
    "            \n",
    "            #the input variables are first define as placeholder \n",
    "            # a placeholder is a variable/data which will be assigned later \n",
    "            # image vector & label\n",
    "            x = tf.placeholder(\"float\", [None, input_size])   # CIFAR data image of shape 3072\n",
    "            y = tf.placeholder(\"float\", [None, output_size])  # 0-9 digits recognition\n",
    "\n",
    "            #the network is defined using the inference function defined above in the code\n",
    "            output = inference(x, desired_layer)\n",
    "\n",
    "            cost = loss_2(output, y)\n",
    "            \n",
    "            #initialize the value of the global_step variable \n",
    "            # recall: it is incremented by one each time the .minimise() is called\n",
    "            global_step = tf.Variable(0, name='global_step', trainable=False)\n",
    "            \n",
    "            train_op = training(cost, global_step)\n",
    "            #train_op = training(cost, global_step=None)\n",
    "            \n",
    "            #evaluate the accuracy of the network (done on a validation set)\n",
    "            eval_op = evaluate(output, y)\n",
    "\n",
    "            summary_op = tf.summary.merge_all()\n",
    "    \n",
    "            #save and restore variables to and from checkpoints.\n",
    "            saver = tf.train.Saver()\n",
    "    \n",
    "            #defines a session\n",
    "            sess = tf.Session()\n",
    "            \n",
    "            # summary writer\n",
    "            #https://www.tensorflow.org/api_docs/python/tf/summary/FileWriter\n",
    "            #\n",
    "            summary_writer = tf.summary.FileWriter(log_files_path+'multi_layer/', sess.graph)\n",
    "        \n",
    "            #initialization of all the variables\n",
    "            init_op = tf.global_variables_initializer()\n",
    "            sess.run(init_op)\n",
    "        \n",
    "            #will work with this later\n",
    "            #saver.restore(sess, log_files_path+'multi_layer/model-checkpoint-66000')\n",
    "            \n",
    "            loss_trace = []\n",
    "\n",
    "            # Training cycle\n",
    "            for epoch in range(training_epochs):\n",
    "\n",
    "                avg_cost = 0.\n",
    "                total_batch = int(x_train.shape[0]/batch_size)\n",
    "            \n",
    "                # Loop over all batches\n",
    "                for i in range(total_batch):\n",
    "\n",
    "                    minibatch_x, minibatch_y = next_batch(batch_size, x_train, y_train)\n",
    "                    \n",
    "                    # Fit training using batch data\n",
    "                    #the training is done using the training dataset\n",
    "                    sess.run(train_op, feed_dict={x: minibatch_x, y: minibatch_y})\n",
    "                    \n",
    "                    # Compute average loss\n",
    "                    avg_cost += sess.run(cost, feed_dict={x: minibatch_x, y: minibatch_y})/total_batch\n",
    "                    \n",
    "                # Display logs per epoch step\n",
    "                if epoch % display_step == 0:\n",
    "                    \n",
    "                    #the accuracy is evaluated using the validation dataset\n",
    "                    accuracy = sess.run(eval_op, feed_dict={x: x_validation, y: y_validation})\n",
    "                    loss_trace.append(1-accuracy)    \n",
    "                    print(\"Epoch:\", '%03d' % epoch, \"cost function=\", \"{:0.7f}\".format(avg_cost), \" Validation Error:\", (1.0 - accuracy))\n",
    "                    summary_str = sess.run(summary_op, feed_dict={x: minibatch_x, y: minibatch_y})\n",
    "                    summary_writer.add_summary(summary_str, sess.run(global_step))\n",
    "                        \n",
    "                    #save to use later\n",
    "                    #https://www.tensorflow.org/api_docs/python/tf/train/Saver\n",
    "                    #saver.save(sess, log_files_path+'model-checkpoint', global_step=global_step)\n",
    "                    saver.save(sess, log_files_path + 'multi_layer/model-checkpoint', global_step=global_step)\n",
    "                        \n",
    "            print(\"Optimization Finished!\")\n",
    "            #accuracy evaluated with the whole test dataset\n",
    "            accuracy = sess.run(eval_op, feed_dict={x: x_test2, y: y_test})\n",
    "            print(\"Test Accuracy:\", accuracy)\n",
    "                    \n",
    "            elapsed_time = time.time() - start_time\n",
    "            print('Execution time (seconds) was %0.3f' % elapsed_time)\n",
    "            \n",
    "            # Visualization of the results\n",
    "            # loss function\n",
    "            plt.plot(loss_trace)\n",
    "            plt.title('Cross Entropy Loss')\n",
    "            plt.xlabel('epoch')\n",
    "            plt.ylabel('loss')\n",
    "            plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weight Matrix: <tf.Variable 'multi_layer/hidden_layer_1/W:0' shape=(3072, 200) dtype=float32_ref>\n",
      "Bias Vector: <tf.Variable 'multi_layer/hidden_layer_1/b:0' shape=(200,) dtype=float32_ref>\n",
      "Weight Matrix: <tf.Variable 'multi_layer/hidden_layer_2/W:0' shape=(200, 200) dtype=float32_ref>\n",
      "Bias Vector: <tf.Variable 'multi_layer/hidden_layer_2/b:0' shape=(200,) dtype=float32_ref>\n",
      "Weight Matrix: <tf.Variable 'multi_layer/hidden_layer_3/W:0' shape=(200, 200) dtype=float32_ref>\n",
      "Bias Vector: <tf.Variable 'multi_layer/hidden_layer_3/b:0' shape=(200,) dtype=float32_ref>\n",
      "Weight Matrix: <tf.Variable 'multi_layer/hidden_layer_4/W:0' shape=(200, 200) dtype=float32_ref>\n",
      "Bias Vector: <tf.Variable 'multi_layer/hidden_layer_4/b:0' shape=(200,) dtype=float32_ref>\n",
      "Weight Matrix: <tf.Variable 'multi_layer/hidden_layer_5/W:0' shape=(200, 200) dtype=float32_ref>\n",
      "Bias Vector: <tf.Variable 'multi_layer/hidden_layer_5/b:0' shape=(200,) dtype=float32_ref>\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "None values not supported.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-139-e1c46597333c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m             \u001b[0;31m#the network is defined using the inference function defined above in the code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minference\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdesired_layer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m             \u001b[0mcost\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-138-2cf02f11f422>\u001b[0m in \u001b[0;36minference\u001b[0;34m(x, desired_layer)\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvariable_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m             \u001b[0mhidden\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mn_hidden\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_hidden\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mn_hidden\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m             \u001b[0;31m#print([input_size, n_hidden_1])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-134-fc3bba2378b3>\u001b[0m in \u001b[0;36mlayer\u001b[0;34m(x, weight_shape, bias_shape, activation)\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mW\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mactivation\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;34m'tanh'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtanh\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mW\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mactivation\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;34m'sigmoid'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msigmoid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mW\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/3point6/lib/python3.7/site-packages/tensorflow/python/util/dispatch.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    178\u001b[0m     \u001b[0;34m\"\"\"Call target, and fall back on dispatchers if there is a TypeError.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 180\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    181\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m       \u001b[0;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/3point6/lib/python3.7/site-packages/tensorflow/python/ops/math_ops.py\u001b[0m in \u001b[0;36mmatmul\u001b[0;34m(a, b, transpose_a, transpose_b, adjoint_a, adjoint_b, a_is_sparse, b_is_sparse, name)\u001b[0m\n\u001b[1;32m   2579\u001b[0m         \u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_to_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"b\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2580\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2581\u001b[0;31m       \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_to_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"a\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2582\u001b[0m       \u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_to_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"b\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2583\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/3point6/lib/python3.7/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mconvert_to_tensor\u001b[0;34m(value, dtype, name, preferred_dtype, dtype_hint)\u001b[0m\n\u001b[1;32m   1085\u001b[0m   preferred_dtype = deprecation.deprecated_argument_lookup(\n\u001b[1;32m   1086\u001b[0m       \"dtype_hint\", dtype_hint, \"preferred_dtype\", preferred_dtype)\n\u001b[0;32m-> 1087\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mconvert_to_tensor_v2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreferred_dtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1088\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1089\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/3point6/lib/python3.7/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mconvert_to_tensor_v2\u001b[0;34m(value, dtype, dtype_hint, name)\u001b[0m\n\u001b[1;32m   1143\u001b[0m       \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1144\u001b[0m       \u001b[0mpreferred_dtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype_hint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1145\u001b[0;31m       as_ref=False)\n\u001b[0m\u001b[1;32m   1146\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1147\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/3point6/lib/python3.7/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36minternal_convert_to_tensor\u001b[0;34m(value, dtype, name, as_ref, preferred_dtype, ctx, accept_symbolic_tensors, accept_composite_tensors)\u001b[0m\n\u001b[1;32m   1222\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1223\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mret\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1224\u001b[0;31m       \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconversion_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mas_ref\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mas_ref\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1225\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1226\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mret\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNotImplemented\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/3point6/lib/python3.7/site-packages/tensorflow/python/framework/constant_op.py\u001b[0m in \u001b[0;36m_constant_tensor_conversion_function\u001b[0;34m(v, dtype, name, as_ref)\u001b[0m\n\u001b[1;32m    303\u001b[0m                                          as_ref=False):\n\u001b[1;32m    304\u001b[0m   \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mas_ref\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 305\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mconstant\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    306\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    307\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/3point6/lib/python3.7/site-packages/tensorflow/python/framework/constant_op.py\u001b[0m in \u001b[0;36mconstant\u001b[0;34m(value, dtype, shape, name)\u001b[0m\n\u001b[1;32m    244\u001b[0m   \"\"\"\n\u001b[1;32m    245\u001b[0m   return _constant_impl(value, dtype, shape, name, verify_shape=False,\n\u001b[0;32m--> 246\u001b[0;31m                         allow_broadcast=True)\n\u001b[0m\u001b[1;32m    247\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    248\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/3point6/lib/python3.7/site-packages/tensorflow/python/framework/constant_op.py\u001b[0m in \u001b[0;36m_constant_impl\u001b[0;34m(value, dtype, shape, name, verify_shape, allow_broadcast)\u001b[0m\n\u001b[1;32m    282\u001b[0m       tensor_util.make_tensor_proto(\n\u001b[1;32m    283\u001b[0m           \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverify_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverify_shape\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 284\u001b[0;31m           allow_broadcast=allow_broadcast))\n\u001b[0m\u001b[1;32m    285\u001b[0m   \u001b[0mdtype_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mattr_value_pb2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAttrValue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtensor_value\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    286\u001b[0m   const_tensor = g.create_op(\n",
      "\u001b[0;32m~/anaconda2/envs/3point6/lib/python3.7/site-packages/tensorflow/python/framework/tensor_util.py\u001b[0m in \u001b[0;36mmake_tensor_proto\u001b[0;34m(values, dtype, shape, verify_shape, allow_broadcast)\u001b[0m\n\u001b[1;32m    452\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    453\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mvalues\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 454\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"None values not supported.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    455\u001b[0m     \u001b[0;31m# if dtype is provided, forces numpy array to be the type\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m     \u001b[0;31m# provided if possible.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: None values not supported."
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    \n",
    "    start_time = time.time()\n",
    "    #please, make sure you changed for your own path \n",
    "    log_files_path = '/Users/Renaissance/Desktop/'\n",
    "\n",
    "    desired_layer=4\n",
    "    \n",
    "    with tf.Graph().as_default():\n",
    "        \n",
    "        with tf.variable_scope(\"multi_layer\"):\n",
    "            #neural network definition \n",
    "            \n",
    "            #the input variables are first define as placeholder \n",
    "            # a placeholder is a variable/data which will be assigned later \n",
    "            # image vector & label\n",
    "            x = tf.placeholder(\"float\", [None, input_size])   # CIFAR data image of shape 3072\n",
    "            y = tf.placeholder(\"float\", [None, output_size])  # 0-9 digits recognition\n",
    "\n",
    "            #the network is defined using the inference function defined above in the code\n",
    "            output = inference(x, desired_layer)\n",
    "\n",
    "            cost = loss_2(output, y)\n",
    "            \n",
    "            #initialize the value of the global_step variable \n",
    "            # recall: it is incremented by one each time the .minimise() is called\n",
    "            global_step = tf.Variable(0, name='global_step', trainable=False)\n",
    "            \n",
    "            train_op = training(cost, global_step)\n",
    "            #train_op = training(cost, global_step=None)\n",
    "            \n",
    "            #evaluate the accuracy of the network (done on a validation set)\n",
    "            eval_op = evaluate(output, y)\n",
    "\n",
    "            summary_op = tf.summary.merge_all()\n",
    "    \n",
    "            #save and restore variables to and from checkpoints.\n",
    "            saver = tf.train.Saver()\n",
    "    \n",
    "            #defines a session\n",
    "            sess = tf.Session()\n",
    "            \n",
    "            # summary writer\n",
    "            #https://www.tensorflow.org/api_docs/python/tf/summary/FileWriter\n",
    "            #\n",
    "            summary_writer = tf.summary.FileWriter(log_files_path+'multi_layer/', sess.graph)\n",
    "        \n",
    "            #initialization of all the variables\n",
    "            init_op = tf.global_variables_initializer()\n",
    "            sess.run(init_op)\n",
    "        \n",
    "            #will work with this later\n",
    "            #saver.restore(sess, log_files_path+'multi_layer/model-checkpoint-66000')\n",
    "            \n",
    "            loss_trace = []\n",
    "\n",
    "            # Training cycle\n",
    "            for epoch in range(training_epochs):\n",
    "\n",
    "                avg_cost = 0.\n",
    "                total_batch = int(x_train.shape[0]/batch_size)\n",
    "            \n",
    "                # Loop over all batches\n",
    "                for i in range(total_batch):\n",
    "\n",
    "                    minibatch_x, minibatch_y = next_batch(batch_size, x_train, y_train)\n",
    "                    \n",
    "                    # Fit training using batch data\n",
    "                    #the training is done using the training dataset\n",
    "                    sess.run(train_op, feed_dict={x: minibatch_x, y: minibatch_y})\n",
    "                    \n",
    "                    # Compute average loss\n",
    "                    avg_cost += sess.run(cost, feed_dict={x: minibatch_x, y: minibatch_y})/total_batch\n",
    "                    \n",
    "                # Display logs per epoch step\n",
    "                if epoch % display_step == 0:\n",
    "                    \n",
    "                    #the accuracy is evaluated using the validation dataset\n",
    "                    accuracy = sess.run(eval_op, feed_dict={x: x_validation, y: y_validation})\n",
    "                    loss_trace.append(1-accuracy)    \n",
    "                    print(\"Epoch:\", '%03d' % epoch, \"cost function=\", \"{:0.7f}\".format(avg_cost), \" Validation Error:\", (1.0 - accuracy))\n",
    "                    summary_str = sess.run(summary_op, feed_dict={x: minibatch_x, y: minibatch_y})\n",
    "                    summary_writer.add_summary(summary_str, sess.run(global_step))\n",
    "                        \n",
    "                    #save to use later\n",
    "                    #https://www.tensorflow.org/api_docs/python/tf/train/Saver\n",
    "                    #saver.save(sess, log_files_path+'model-checkpoint', global_step=global_step)\n",
    "                    saver.save(sess, log_files_path + 'multi_layer/model-checkpoint', global_step=global_step)\n",
    "                        \n",
    "            print(\"Optimization Finished!\")\n",
    "            #accuracy evaluated with the whole test dataset\n",
    "            accuracy = sess.run(eval_op, feed_dict={x: x_test2, y: y_test})\n",
    "            print(\"Test Accuracy:\", accuracy)\n",
    "                    \n",
    "            elapsed_time = time.time() - start_time\n",
    "            print('Execution time (seconds) was %0.3f' % elapsed_time)\n",
    "            \n",
    "            # Visualization of the results\n",
    "            # loss function\n",
    "            #plt.plot(loss_trace)\n",
    "            #plt.title('Cross Entropy Loss')\n",
    "            #plt.xlabel('epoch')\n",
    "            #plt.ylabel('loss')\n",
    "            #plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
